{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demographic characteristics are important input characteristics of various recommendation systems, which naturally also include advertising platforms. Most verification methods use demographic attributes as input to generate recommendation results, and then compare the recommendation performance with and without these inputs offline or online. It worth to attempts to verify this hypothesis from another direction, that is, to use the user's interaction in the advertising system as input to predict the user's demographic attributes.**\n",
    "\n",
    "**This kernal used to predict gender and age which are very important parts of user portrait.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:Generate user history click sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc \n",
    "#train\n",
    "train_data=pd.read_csv('tencent2020/train_preliminary/click_log.csv')\n",
    "#test\n",
    "test_data=pd.read_csv('tencent2020/test/click_log.csv')\n",
    "#all data\n",
    "data=train_data.append(test_data)\n",
    "data=data.reset_index(drop=True)\n",
    "print(data[:5])\n",
    "print(len(data))\n",
    "del train_data,test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce memory\n",
    "import numpy as np\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "data=reduce_mem_usage(data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "cols=['creative_id']\n",
    "for feat in tqdm(cols):\n",
    "    lbe = LabelEncoder()  # or Hash\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_session\n",
    "from joblib import Parallel, delayed\n",
    "def gen_session_list(user_id, t):\n",
    "    t.sort_values('time', inplace=True, ascending=True)\n",
    "    session = []\n",
    "    for row in t.iterrows():\n",
    "        creative= row[1]['creative_id']\n",
    "        session.append((creative))\n",
    "    return user_id, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(df_grouped, func, n_jobs, backend='multiprocessing'):\n",
    "    \"\"\"Use Parallel and delayed \"\"\"  # backend='threading'\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=4, backend=backend)(delayed(func)(name, group) for name, group in df_grouped)\n",
    "    return {k: v for k, v in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user=pd.DataFrame()\n",
    "all_user['user_id']=data['user_id']\n",
    "all_user=all_user.drop_duplicates()\n",
    "all_user=all_user.reset_index(drop=True)\n",
    "print(len(all_user))\n",
    "print(all_user[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = all_user.shape[0]\n",
    "print(n_samples)\n",
    "batch_size = 500000\n",
    "iters = (n_samples - 1) // batch_size + 1\n",
    "print(\"total\", iters, \"iters\", \"batch_size\", batch_size)\n",
    "for i in range(0, iters):\n",
    "    target_user = all_user['user_id'].values[i * batch_size:(i + 1) * batch_size]\n",
    "    sub_data = data.loc[data.user_id.isin(target_user)]\n",
    "    print(i, 'iter start')\n",
    "    df_grouped = sub_data.groupby(['user_id'])\n",
    "    user_hist_session = applyParallel(df_grouped, gen_session_list, n_jobs=20, backend='loky')\n",
    "    pd.to_pickle(user_hist_session, 'tencent2020/user_hist_session' +str(i)+'.pkl')\n",
    "    print(i, 'pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the hist_session\n",
    "import pandas as pd\n",
    "import gc\n",
    "user_hist_session = {}\n",
    "for i in range(4):\n",
    "  user_hist_session_= pd.read_pickle('tencent2020/user_hist_session' + str(i) + '.pkl')\n",
    "  user_hist_session.update(user_hist_session_)\n",
    "  del user_hist_session_\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user=pd.read_csv('tencent2020/train_preliminary/user.csv')\n",
    "test_user=pd.read_csv('tencent2020/test/click_log.csv')\n",
    "test_user=test_user[['user_id']]\n",
    "test_user=test_user.drop_duplicates()\n",
    "test_user=test_user.reset_index(drop=True)\n",
    "print(len(test_user))\n",
    "print(test_user[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESS_MAX_LEN=64\n",
    "def gen_sess_feature(row):\n",
    "    sess_max_len = SESS_MAX_LEN\n",
    "    sess_input_dict = {'creative':[0]}\n",
    "    sess_input_length = 0\n",
    "    user= row[1]['user_id']\n",
    "    if user not in user_hist_session or len(user_hist_session[user]) == 0:\n",
    "        sess_input_dict['creative'] = [0]\n",
    "        sess_input_length = 0\n",
    "    else:\n",
    "        cur_sess = user_hist_session[user]\n",
    "        for i in reversed(range(len(cur_sess))):\n",
    "            sess_input_dict['creative'] = [e[2] for e in cur_sess[max(0, i + 1 - sess_max_len):i + 1]]\n",
    "            sess_input_length = len(sess_input_dict['creative'])\n",
    "            break\n",
    "    return sess_input_dict['creative'],sess_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "sess_input_dict= {'creative':[]}\n",
    "sess_input_length= []\n",
    "for row in tqdm(train_user[['user_id']].iterrows()):\n",
    "    a, b= gen_sess_feature_din(row)\n",
    "    sess_input_dict['creative'].append(a)\n",
    "    sess_input_length.append(b)\n",
    "print('done')\n",
    "train_user['creative']=sess_input_dict['creative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_input_dict= {'creative':[]}\n",
    "sess_input_length= []\n",
    "for row in tqdm(test_user[['user_id']].iterrows()):\n",
    "    a, b= gen_sess_feature_din(row)\n",
    "    sess_input_dict['creative'].append(a)\n",
    "    sess_input_length.append(b)\n",
    "print('done')\n",
    "test_user['creative']=sess_input_dict['creative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "train_input=[pad_sequences(train_user['creative'].values, maxlen=SESS_MAX_LEN, padding='post')]\n",
    "test_input=[pad_sequences(test_user['creative'].values, maxlen=SESS_MAX_LEN, padding='post')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train_input, 'tecent2020/train_input_creative_64' +'.pkl')\n",
    "pd.to_pickle(test_input, 'tecent2020/test_input_creative_64' +'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3:BILSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_id_max=len(set(data.creative_id)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant,RandomNormal\n",
    "from keras.regularizers import l2\n",
    "SESS_MAX_LEN=64\n",
    "em=128\n",
    "model = Sequential()\n",
    "model.add(Embedding(creative_id_max+1,\n",
    "                    em,\n",
    "                    embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n",
    "                    embeddings_regularizer=l2(1e-6),\n",
    "                    input_length=SESS_MAX_LEN,\n",
    "                    trainable=True,\n",
    "                    mask_zero=True))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4: BILSTM with multi inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import Constant,RandomNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "SESS_MAX_LEN=64\n",
    "em=128\n",
    "\n",
    "inputA = Input(shape=(SESS_MAX_LEN,))\n",
    "inputB = Input(shape=(SESS_MAX_LEN,))\n",
    "inputC = Input(shape=(SESS_MAX_LEN,))\n",
    "\n",
    "x=Embedding(ad_id_max+1,\n",
    "            em,\n",
    "            embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n",
    "            embeddings_regularizer=l2(1e-6),\n",
    "            input_length=SESS_MAX_LEN,\n",
    "            trainable=True,\n",
    "            mask_zero=True)(inputA)\n",
    "x=SpatialDropout1D(0.2)(x)\n",
    "x=Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x=Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x_avg_pool = GlobalAveragePooling1D()(x)\n",
    "# x_avg_pool = GlobalMaxPooling1D()(x)\n",
    "# x_conc= concatenate([x_avg_pool,x_max_pool])\n",
    "x_last=Dropout(0.25)(x_avg_pool)\n",
    "x_last= Model(inputs=inputA, outputs=x_last)\n",
    "\n",
    "\n",
    "y=Embedding(advertiser_id_max+1,\n",
    "            em,\n",
    "            embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n",
    "            embeddings_regularizer=l2(1e-6),\n",
    "            input_length=SESS_MAX_LEN,\n",
    "            trainable=True,\n",
    "            mask_zero=True)(inputB)\n",
    "y=SpatialDropout1D(0.2)(y)\n",
    "y=Bidirectional(LSTM(128, return_sequences=True))(y)\n",
    "y=Bidirectional(LSTM(64, return_sequences=True))(y)\n",
    "y_avg_pool = GlobalAveragePooling1D()(y)\n",
    "# y_avg_pool = GlobalMaxPooling1D()(y)\n",
    "# y_conc= concatenate([y_avg_pool,y_max_pool])\n",
    "y_last=Dropout(0.25)(y_avg_pool)\n",
    "y_last= Model(inputs=inputB, outputs=y_last)\n",
    "\n",
    "z=Embedding(creative_id_max+1,\n",
    "            em,\n",
    "            embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=2020),\n",
    "            embeddings_regularizer=l2(1e-6),\n",
    "            input_length=SESS_MAX_LEN,\n",
    "            trainable=True,\n",
    "            mask_zero=True)(inputC)\n",
    "z=SpatialDropout1D(0.2)(z)\n",
    "z=Bidirectional(LSTM(128, return_sequences=True))(z)\n",
    "z=Bidirectional(LSTM(64, return_sequences=True))(z)\n",
    "z_avg_pool = GlobalAveragePooling1D()(z)\n",
    "# z_avg_pool = GlobalMaxPooling1D()(z)\n",
    "# z_conc= concatenate([z_avg_pool,z_max_pool])\n",
    "z_last=Dropout(0.25)(z_avg_pool)\n",
    "z_last= Model(inputs=inputC, outputs=z_last)\n",
    "\n",
    "# combine the output of the three branches\n",
    "combined = concatenate([x_last.output, y_last.output,z_last.output])\n",
    "\n",
    "w = Dense(64, activation=\"relu\")(combined)\n",
    "w = Dropout(0.25)(w)\n",
    "w1 = Dense(2, activation=\"softmax\")(w)\n",
    "w2 = Dense(10, activation=\"softmax\")(w)\n",
    "\n",
    "# our model will accept the inputs of the three branches and\n",
    "# then output two values\n",
    "model = Model(inputs=[inputA, inputB,inputC], outputs=[w1,w2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4:transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "embed_size = 128\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        outputs = Add()([outputs, q])\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "class DecoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_att_layer  = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, dec_input, enc_output, self_mask=None, enc_mask=None):\n",
    "        output, slf_attn = self.self_att_layer(dec_input, dec_input, dec_input, mask=self_mask)\n",
    "        output, enc_attn = self.enc_att_layer(output, enc_output, enc_output, mask=enc_mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn, enc_attn\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n",
    "class Encoder():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, \\\n",
    "                layers=6, dropout=0.1, word_emb=None, pos_emb=None):\n",
    "        self.emb_layer = word_emb\n",
    "        self.pos_layer = pos_emb\n",
    "        self.emb_dropout = Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout) for _ in range(layers)]\n",
    "        \n",
    "    def __call__(self, src_seq, src_pos, return_att=False, active_layers=999):\n",
    "        x = self.emb_layer(src_seq)\n",
    "        if src_pos is not None:\n",
    "            pos = self.pos_layer(src_pos)\n",
    "            x = Add()([x, pos])\n",
    "        x = self.emb_dropout(x)\n",
    "        if return_att: atts = []\n",
    "        mask = Lambda(lambda x:GetPadMask(x, x))(src_seq)\n",
    "        for enc_layer in self.layers[:active_layers]:\n",
    "            x, att = enc_layer(x, mask)\n",
    "            if return_att: atts.append(att)\n",
    "        return (x, atts) if return_att else x\n",
    "\n",
    "\n",
    "class Transformer():\n",
    "    def __init__(self, len_limit, d_model=embed_size, \\\n",
    "              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n",
    "              share_word_emb=False, **kwargs):\n",
    "        self.name = 'Transformer'\n",
    "        self.len_limit = len_limit\n",
    "        self.src_loc_info = True\n",
    "        self.d_model = d_model\n",
    "        self.decode_model = None\n",
    "        d_emb = d_model\n",
    "\n",
    "        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n",
    "                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n",
    "\n",
    "        i_word_emb = Embedding(max_features, d_emb) # Add embedding here\n",
    "\n",
    "        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n",
    "                               word_emb=i_word_emb, pos_emb=pos_emb)\n",
    "\n",
    "        \n",
    "    def get_pos_seq(self, x):\n",
    "        mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "        return pos * mask\n",
    "\n",
    "    def compile(self, active_layers=999):\n",
    "        src_seq_input = Input(shape=(None,))\n",
    "        src_seq = src_seq_input\n",
    "        src_pos = Lambda(self.get_pos_seq)(src_seq)\n",
    "        if not self.src_loc_info: src_pos = None\n",
    "\n",
    "        x = self.encoder(src_seq, src_pos, active_layers=active_layers)\n",
    "        # x = GlobalMaxPool1D()(x) # Not sure about this layer. Just wanted to reduce dimension\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        outp = Dense(2, activation=\"softmax\")(x)\n",
    "        self.model = Model(inputs=src_seq_input, outputs=outp)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "maxlen=SESS_MAX_LEN\n",
    "max_features=creative_id_max+1\n",
    "model= Transformer(maxlen, layers=1)\n",
    "model.compile()\n",
    "model =model.model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part5:Transformer with gradient penalty and customized attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "embed_size = 128\n",
    "heads=8\n",
    "deep=embed_size//heads\n",
    "maxlen=64\n",
    "drop=0.2\n",
    "p=0.2\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=drop):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:x)(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        outputs = Add()([outputs, q])\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=drop):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=drop):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "class DecoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=drop):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_att_layer  = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, dec_input, enc_output, self_mask=None, enc_mask=None):\n",
    "        output, slf_attn = self.self_att_layer(dec_input, dec_input, dec_input, mask=self_mask)\n",
    "        output, enc_attn = self.enc_att_layer(output, enc_output, enc_output, mask=enc_mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn, enc_attn\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    # mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.cast(K.expand_dims(k, 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n",
    "class Encoder():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, \\\n",
    "                layers=6, dropout=drop, ad_emb=None,adv_emb=None,ind_emb=None,creative_emb=None,pos_emb=None):\n",
    "        self.ad_emb_layer = ad_emb\n",
    "        self.adv_emb_layer = adv_emb\n",
    "        self.ind_emb_layer = ind_emb\n",
    "        self.creative_emb_layer = creative_emb\n",
    "        self.pos_layer = pos_emb\n",
    "        self.emb_dropout = Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout) for _ in range(layers)]\n",
    "        \n",
    "    def __call__(self,ad_seq,adv_seq,ind_seq,creative_seq,click_seq,src_pos,return_att=False, active_layers=999):\n",
    "        ad = self.ad_emb_layer(ad_seq)\n",
    "        adv = self.adv_emb_layer(adv_seq)\n",
    "        ind = self.ind_emb_layer(ind_seq)\n",
    "        creative = self.creative_emb_layer(creative_seq)\n",
    "    \n",
    "        # x = Add()([ad,adv])\n",
    "        # x = Add()([x,ind])\n",
    "\n",
    "        if src_pos is not None:\n",
    "            pos = self.pos_layer(src_pos)\n",
    "            ad = Add()([ad, pos])\n",
    "            adv= Add()([adv, pos])\n",
    "            ind= Add()([ind, pos])\n",
    "            creative= Add()([creative, pos])\n",
    "        x=keras.layers.concatenate([ad, adv, ind,creative])\n",
    "        x=keras.layers.Dense(embed_size)(x)\n",
    "        x=Dropout(0.25)(x)\n",
    "        x = self.emb_dropout(x)\n",
    "        if return_att: atts = []\n",
    "        # m_click=keras.layers.concatenate([click_seq, click_seq, click_seq])\n",
    "        mask = Lambda(lambda x:GetPadMask(x, x))(click_seq)\n",
    "        for enc_layer in self.layers[:active_layers]:\n",
    "            x, att = enc_layer(x, mask)\n",
    "            if return_att: atts.append(att)\n",
    "        return (x, atts) if return_att else x\n",
    "\n",
    "\n",
    "class Transformer():\n",
    "    def __init__(self, len_limit=maxlen, d_model=embed_size, \\\n",
    "              d_inner_hid=512, n_head=heads, d_k=deep, d_v=deep, layers=2, dropout=drop, \\\n",
    "              share_word_emb=False, **kwargs):\n",
    "        self.name = 'Transformer'\n",
    "        self.len_limit = len_limit\n",
    "        self.src_loc_info = True\n",
    "        self.d_model = d_model\n",
    "        self.decode_model = None\n",
    "        d_emb = d_model\n",
    "\n",
    "        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n",
    "                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n",
    "\n",
    "        ad_word_emb = Embedding(ad_id_max+1, d_emb,name='ad_em') # Add embedding here\n",
    "        adv_word_emb = Embedding(advertiser_id_max+1, d_emb,name='adv_em') # Add embedding here\n",
    "        ind_word_emb = Embedding(industry_id_max+1, d_emb,name='ind_em') # Add embedding here\n",
    "        creative_word_emb = Embedding(creative_id_max+1, d_emb,name='creative_em')\n",
    "\n",
    "        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n",
    "                               ad_emb=ad_word_emb, adv_emb=adv_word_emb,ind_emb=ind_word_emb,creative_emb=creative_word_emb,pos_emb=pos_emb)\n",
    "\n",
    "        \n",
    "    def get_pos_seq(self, x):\n",
    "        mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "        return pos * mask\n",
    "\n",
    "\n",
    "    def search_layer(self,inputs, name, exclude=None):\n",
    "        if exclude is None:\n",
    "            exclude = set()\n",
    "        if isinstance(inputs, keras.layers.Layer):\n",
    "            layer = inputs\n",
    "        else:\n",
    "            layer = inputs._keras_history[0]\n",
    "\n",
    "        if layer.name == name:\n",
    "            return layer\n",
    "        elif layer in exclude:\n",
    "            return None\n",
    "        else:\n",
    "            exclude.add(layer)\n",
    "            inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "            if not isinstance(inbound_layers, list):\n",
    "                inbound_layers = [inbound_layers]\n",
    "            if len(inbound_layers) > 0:\n",
    "                  for layer in inbound_layers:\n",
    "                        layer = self.search_layer(layer, name, exclude)\n",
    "                        if layer is not None:\n",
    "                             return layer\n",
    "\n",
    "\n",
    "    def loss_with_gradient_penalty(self,y_true, y_pred, epsilon=1):\n",
    "\n",
    "        loss = K.mean(categorical_crossentropy(y_true, y_pred))\n",
    "         ad_embeddings = self.search_layer(y_pred, 'ad_em').embeddings\n",
    "      # adv_embeddings = self.search_layer(y_pred, 'adv_em').embeddings\n",
    "      # ind_embeddings = self.search_layer(y_pred, 'ind_em').embeddings\n",
    "         ad_gp = K.sum(K.gradients(loss, [ad_embeddings])[0].values**2)\n",
    "      # adv_gp = K.sum(K.gradients(loss, [adv_embeddings])[0].values**2)\n",
    "      # ind_gp = K.sum(K.gradients(loss, [ind_embeddings])[0].values**2)\n",
    "         return loss + p*epsilon *ad_gp \n",
    "\n",
    "    def compile(self, active_layers=999):\n",
    "        ad_seq= Input(shape=(maxlen,))\n",
    "        adv_seq= Input(shape=(maxlen,))\n",
    "        ind_seq= Input(shape=(maxlen,))\n",
    "        creative_seq= Input(shape=(maxlen,))\n",
    "        click_seq= Input(shape=(maxlen,))\n",
    "        src_seq=ad_seq\n",
    "        src_pos = Lambda(self.get_pos_seq)(src_seq)\n",
    "        if not self.src_loc_info: src_pos = None\n",
    "        x = self.encoder(ad_seq,adv_seq,ind_seq,creative_seq,click_seq,src_pos,active_layers=active_layers)\n",
    "        x=Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        x=Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "        x=GlobalMaxPool1D()(x) # Not sure about this layer. Just wanted to reduce dimension\n",
    "        x=Dropout(0.25)(x)\n",
    "        # x = GlobalAveragePooling1D()(x)\n",
    "        # outp1 = Dense(10, activation=\"softmax\")(x)\n",
    "        # outp2 = Dense(2, activation=\"softmax\")(x)\n",
    "        outp= Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = Model(inputs=[ad_seq,adv_seq,ind_seq,creative_seq,click_seq], outputs=outp)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part6:Reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(train_input_ad,train_input_advertiser,train_input_creative,train_set,target,batch_size):\n",
    "\n",
    "    x_samples_ad=train_input_ad[0]\n",
    "    x_samples_advertiser=train_input_advertiser[0]\n",
    "    x_samples_creative=train_input_creative[0]\n",
    "    y_samples=pd.get_dummies(train_set[target]).values\n",
    "\n",
    "    batch_num = int(len(train_set) / batch_size)\n",
    "    max_len = batch_num * batch_size\n",
    "    x_samples_ad = x_samples_ad[:max_len]\n",
    "    x_samples_advertiser = x_samples_advertiser[:max_len]\n",
    "    x_samples_creative= x_samples_creative[:max_len]\n",
    "    y_samples = y_samples[:max_len]\n",
    " \n",
    "    print('the length of samples:', len(y_samples))\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        x1=x_samples_ad[i * batch_size:(i + 1) * batch_size]\n",
    "        x2=x_samples_advertiser[i * batch_size:(i + 1) * batch_size]\n",
    "        x3=x_samples_creative[i * batch_size:(i + 1) * batch_size]\n",
    "        y=y_samples[i * batch_size:(i + 1) * batch_size]\n",
    "        yield ([x1,x2,x3],y)\n",
    "        \n",
    " \n",
    "batch_size = 2048\n",
    "model.fit_generator(\n",
    "  get_train(train_input_ad,train_input_advertiser,train_input_creative,train_set,'gender',batch_size=batch_size),\n",
    "  epochs=10,\n",
    "  steps_per_epoch=int(len(train_set) / batch_size),\n",
    "  max_queue_size=300,\n",
    "  validation_split=0.2，\n",
    "shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
